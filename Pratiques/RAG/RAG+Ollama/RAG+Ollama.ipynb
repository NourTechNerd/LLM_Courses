{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load our gaming PDFs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"/teamspace/studios/this_studio/LLM_Courses/Pratiques/RAG+Ollama/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents(DATA_PATH):\n",
    "    document_loader = PyPDFDirectoryLoader(DATA_PATH)\n",
    "    return document_loader.load()\n",
    "\n",
    "def split_documents(documents):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=800,\n",
    "        chunk_overlap=80,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "    return text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pages = load_documents(DATA_PATH)\n",
    "Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = split_documents(Pages)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Function\n",
    "\n",
    "- We should use the same function for creating the database and for embed the prompt !!\n",
    "- There are alot of models ready to use thanks to langChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings.ollama import OllamaEmbeddings\n",
    "\n",
    "def Embedding_function():\n",
    "    embeddings = OllamaEmbeddings(model=\"nomic-embed-text\",show_progress =True)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Chroma DataBase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHROMA_PATH = \"Chroma\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_database():\n",
    "    if os.path.exists(CHROMA_PATH):\n",
    "        shutil.rmtree(CHROMA_PATH)\n",
    "        \n",
    "def save_to_chroma(chunks,Ids): \n",
    "    clear_database()\n",
    "    # Create a new DB from the chunks.\n",
    "    db = Chroma.from_documents(chunks, Embedding_function(), persist_directory=CHROMA_PATH,ids=Ids)\n",
    "    db.persist() # Forcing the Save\n",
    "    print(f\"Saved {len(chunks)} chunks to {CHROMA_PATH}.\")\n",
    "\n",
    "\n",
    "def Create_Chunks_Ids(chunks):\n",
    "\n",
    "    # This will create IDs like \"data/monopoly.pdf:6:2\"\n",
    "    # Page Source : Page Number : Chunk Index\n",
    "\n",
    "    last_page_id = None\n",
    "    current_chunk_index = 0\n",
    "    ids = []\n",
    "    for chunk in chunks:\n",
    "        source = chunk.metadata.get(\"source\")\n",
    "        page = chunk.metadata.get(\"page\")\n",
    "        current_page_id = f\"{source}:{page}\"\n",
    "\n",
    "        # If the page ID is the same as the last one, increment the index.\n",
    "        if current_page_id == last_page_id:\n",
    "            current_chunk_index += 1\n",
    "        else:\n",
    "            current_chunk_index = 0\n",
    "\n",
    "        # Chunk Id\n",
    "        chunk_id = f\"{current_page_id}:{current_chunk_index}\"\n",
    "        last_page_id = current_page_id\n",
    "\n",
    "        # Add it to the page meta-data.\n",
    "        chunk.metadata[\"id\"] = chunk_id\n",
    "        ids.append(chunk_id)\n",
    "    return chunks,ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks,ids = Create_Chunks_Ids(chunks)\n",
    "chunks[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_chroma(chunks,ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update The DataBase\n",
    "\n",
    "- We want a technic that update the Chroma Database whidout recreate it again each time we add or remove a document ??\n",
    "  - Easy , we will create a unique id for each embedding vector in the database, made from the **Page Source : Page Number : Chunk Index**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma(persist_directory=CHROMA_PATH, embedding_function=Embedding_function())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Existing_ids():\n",
    "    existing_items = db.get(include=[])  # IDs are always included by default\n",
    "    existing_ids = set(existing_items[\"ids\"])\n",
    "    print(f\"Number of existing chunks in DB: {len(existing_ids)}\")\n",
    "    #print(existing_items)\n",
    "    return existing_ids\n",
    "\n",
    "\n",
    "def Update_Chroma(Chunks):\n",
    "    existing_ids = Get_Existing_ids()\n",
    "    Chunks,_       = Create_Chunks_Ids(Chunks)\n",
    "    # Only add documents that don't exist in the DB.\n",
    "    new_chunks = []\n",
    "    for chunk in Chunks:\n",
    "        if chunk.metadata[\"id\"] not in existing_ids:\n",
    "            new_chunks.append(chunk)\n",
    "\n",
    "    if len(new_chunks):\n",
    "        print(f\"ðŸ‘‰ Adding new chunks: {len(new_chunks)}\")\n",
    "        new_chunk_ids = [chunk.metadata[\"id\"] for chunk in new_chunks]\n",
    "        db.add_documents(new_chunks, ids=new_chunk_ids)\n",
    "        db.persist()\n",
    "    else:\n",
    "        print(\"âœ… No new documents to add\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Update_Chroma(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching for relevent chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Answer the question based on the above context: {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_Prompt(Question):\n",
    "    # Searching for Relevent Chunks from DataBase\n",
    "    results = db.similarity_search_with_relevance_scores(Question,k=3)\n",
    "    context_text = \"\\n\\n---\\n\\n\".join([chunk.page_content for chunk, _score in results])\n",
    "    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "    # The Prompt\n",
    "    prompt = prompt_template.format(context=context_text, question=Question)\n",
    "    return prompt,results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt,results = Create_Prompt(\"How do U get out of jail in Monopoly ?\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompting Qween2:1.5b:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llm = Ollama(model=\"qwen2:1.5b\")\n",
    "response = llm.invoke(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sources used by LLM\n",
    "sources = [chunk.metadata.get(\"id\", None) for chunk, _score in results]\n",
    "formatted_response = f\"Response: {response}\\n\\nSources: {sources}\"\n",
    "print(formatted_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality Of RAG System :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its depend on :\n",
    "- The Quality of Information and Documents.\n",
    "- The Text Spliting technic.\n",
    "- The Searching technic (KNN, Clustring...)\n",
    "- The LLM used and the Embedding model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can Build Tests, and let the LLM evaluate itself :\n",
    "- Test like Quetion and Expected response,also we can do the negation that mean give the Quetion and a wrong response and see if the evaluation fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_PROMPT = \"\"\"\n",
    "Actual Response: {actual_response}\n",
    "\n",
    "Expected Response: {expected_response}\n",
    "---\n",
    "(Answer with 'true' or 'false') Does the actual response match the expected response? \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_Evaluation_Prompt(Response,Expected):\n",
    "    prompt_template = ChatPromptTemplate.from_template(EVAL_PROMPT)\n",
    "    # The Prompt\n",
    "    prompt = prompt_template.format(actual_response=Response,expected_response=Expected)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tests =  {\n",
    "    \"How much total money does a player start with in Monopoly? (Answer with the number only)\": \"$1500\",\n",
    "    \"How many points does the longest continuous train get in Ticket to Ride? (Answer with the number only)\": \"10 points\",\n",
    "}\n",
    "def Evaluation(Tests):\n",
    "    for question, answer in Tests.items():\n",
    "        prompt,_ = Create_Prompt(question)\n",
    "        response = llm.invoke(prompt)\n",
    "        #print(response+\"\\n\\n\\n\")\n",
    "        \n",
    "        Evaluation_prompt = Create_Evaluation_Prompt(response,answer)\n",
    "        Evaluation_response = llm.invoke(Evaluation_prompt)\n",
    "        # Clean the Evaluation response\n",
    "        Evaluation = Evaluation_response.strip().lower()\n",
    "        if \"true\" in Evaluation:\n",
    "            print(\"\\033[32mTest Passed\\033[0m\")\n",
    "        else :\n",
    "            print(\"\\033[31mTest Failed\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluation(Tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM_Courses",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
