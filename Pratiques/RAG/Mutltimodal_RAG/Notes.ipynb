{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLIP ?\n",
    "\n",
    "- It's an OpenAI model related in 2021, for the purpose of supervising the generated images by DALLE , the model it samply trained to decide if piece if Text is relevent or not to an Image.\n",
    "- It's trained on a set of 400M image-text pairs.\n",
    "<img style=\"display: block; width: 600px; height: 300px;\" src=\"https://images.ctfassets.net/kftzwdyauwt9/fbc4f633-9ad4-4dc2-3809c22df5e0/0bd2d5abf90d052731538613e4a42668/overview-a.svg\">\n",
    "\n",
    "- The model trained to maximize the diagonal and minimize outherwise.\n",
    "- The modal can be used as a <span style = \"color:#5996f7 ;font-weight:bold\">Image Retriver</span> , you give a text and it returns a relevent image to wht you decsribe.\n",
    "- The modal can be used as a <span style = \"color:#5996f7 ;font-weight:bold\"> Zero-shot Image Classifier</span> , you give ot mutliple texts and an Image it can maximize the score of relevant description.\n",
    "\n",
    "- The modal can be used as a <span style = \"color:#5996f7 ;font-weight:bold\">Image Embedding model</span> , to Embed Images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multimodal RAG :\n",
    "\n",
    "### Why ?\n",
    "\n",
    "- Because Good information it not only on text data, ot can be in Images,Audio,Tables,Charts,etc, so if we can use it it can be good for our system.\n",
    "\n",
    "### Approachs :\n",
    "\n",
    "#### Approach 1 :\n",
    "- We can use like Clip model to get embedding of both text,image then store it in Vector database...,during retrievement step we can pass the relevent content (Text chunks +Images)\n",
    "to VLM to give final response.\n",
    "\n",
    "#### Approach 2 :\n",
    "- Convert Everything to text with VLMs like LLava,Gemeni-flash...,then continue building RAG with only text...\n",
    "\n",
    "### Approach 3 :\n",
    "- Like Approach 2,but we keep reference to Images , so while retrieving we can pass Image +Text to VLMs to get final response.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
