{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application1 : MRAG with Videos :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index-vector-stores-lancedb\n",
    "%pip install llama-index-multi-modal-llms-openai\n",
    "%pip install llama-index-embeddings-clip\n",
    "%pip install git+https://github.com/openai/CLIP.git\n",
    "%pip install llama-index-readers-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama_index\n",
    "%pip install -U openai-whisper # For Transcription of the Video audio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install lancedb # Vector database to store Images and Text embeddings\n",
    "%pip install moviepy # To edit Videos\n",
    "%pip install pytube # Tp Download videos from Youtube\n",
    "%pip install pydub  # To manipulate audio like Loading exporting editing...\n",
    "%pip install SpeechRecognition # Speech to text model\n",
    "%pip install ffmpeg-python \n",
    "%pip install soundfile # For reading and writing audio files in many extensions.\n",
    "%pip install torch torchvision\n",
    "%pip install matplotlib scikit-image\n",
    "%pip install ftfy regex tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ftfy\n",
    "\n",
    "# A string with encoding problems (mojibake)\n",
    "broken_text = \"This is an example of mojibake: cafÃ©, El NiÃ±o.\"\n",
    "\n",
    "# Fixing the broken text\n",
    "fixed_text = ftfy.fix_text(broken_text)\n",
    "\n",
    "print(fixed_text)\n",
    "# Output: \"This is an example of mojibake: café, El Niño.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import VideoFileClip\n",
    "from pathlib import Path\n",
    "import speech_recognition as sr\n",
    "from pytube import YouTube\n",
    "from pprint import pprint\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Specify the absolute path to your .env file\n",
    "dotenv_path = '/teamspace/studios/this_studio/LLM_Courses/Pratiques/RAG/Mutltimodal_RAG/.env'\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "# Get the NVIDIA LLM API key\n",
    "NVIDIA_LLM_API_KEY = os.getenv(\"NVIDIA_LLM_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_url = \"https://youtu.be/dh8Rxhf7cLU\"  # Cleaned URL\n",
    "current_diractory =  \"/teamspace/studios/this_studio/LLM_Courses/Pratiques/RAG/Mutltimodal_RAG\"\n",
    "data_path = os.path.join(current_diractory,\"data\")\n",
    "video_path = os.path.join(data_path,\"Input_vid.mp4\")\n",
    "Images_path = os.path.join(data_path,\"Images\")\n",
    "audio_path = os.path.join(data_path,\"audio.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytube import YouTube\n",
    "from moviepy.editor import VideoFileClip\n",
    "\n",
    "def download_video(url,output_path):\n",
    "  yt = YouTube(url)\n",
    "  metadata = {\"Author\": yt.author, \"Title\": yt.title, \"Views\": yt.views}\n",
    "\n",
    "  yt.streams.get_highest_resolution().download(\n",
    "        output_path=output_path, filename=\"input_vid.mp4\"\n",
    "    )\n",
    "  return metadata\n",
    "\n",
    "def video_to_images(video_path,output_folder):\n",
    "  clip=VideoFileClip(video_path)\n",
    "  clip.write_images_sequence(\n",
    "      os.path.join(output_folder,\"frame%04d.png\"),fps=0.2\n",
    "  )\n",
    "\n",
    "def video_to_audio(video_path,output_audio_path):\n",
    "  clip=VideoFileClip(video_path)\n",
    "  audio=clip.audio\n",
    "  audio.write_audiofile(output_audio_path)\n",
    "\n",
    "def audio_to_text(audio_path):\n",
    "  recognizer=sr.Recognizer()\n",
    "  audio=sr.AudioFile(audio_path)\n",
    "\n",
    "  with audio as source:\n",
    "    audio_data=recognizer.record(source)\n",
    "\n",
    "    try:\n",
    "\n",
    "      #recognize the speech\n",
    "      text = recognizer.recognize_whisper(audio_data)\n",
    "\n",
    "    except sr.UnknownValueError:\n",
    "      print(\"Speech recognition could not understand the audio.\")\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_to_images(video_path,Images_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_to_audio(video_path,output_audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription =audio_to_text(output_audio_path)\n",
    "with open(os.path.join(data_path,\"transcription.txt\"), 'w') as file:\n",
    "    file.write(transcription)\n",
    "print(\"Text data saved to file\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.indices import MultiModalVectorStoreIndex\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.vector_stores.lancedb import LanceDBVectorStore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We Initialize two Vector Stors one for text embeddings and anouther for Images embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_store=LanceDBVectorStore(uri=\"lancedb\",table_name=\"text_collection\")\n",
    "image_store=LanceDBVectorStore(uri=\"lancedb\",table_name=\"image_collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_context=StorageContext.from_defaults(vector_store=text_store,image_store=image_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(Images_path).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = MultiModalVectorStoreIndex.from_documents(documents,storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
