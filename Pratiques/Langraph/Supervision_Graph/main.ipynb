{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úîÔ∏è The idea is to create a Graph with a supervision LLM, that will orchestre the Outher Workers(Agents).\n",
    "\n",
    "‚úîÔ∏è The Cool here is we will bind This LLM with Agents Not Tools ü§©"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup :\n",
    "\n",
    "curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "ollama pull llama3-groq-tool-use:8b\n",
    "\n",
    "ollama pull llama3.1:8b-instruct-fp16\n",
    "\n",
    "ollama pull mistral-nemo:12b-instruct-2407-fp16       \n",
    "\n",
    "ollama pull codegeex4:latest  `Not working with Function calling , but is good at Code related tasks`\n",
    "\n",
    "ollama pull nomic-embed-text  `For Embedding`\n",
    "\n",
    "\n",
    "\n",
    "\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build LLMs and Tools :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import TavilySearchAPIRetriever\n",
    "import os\n",
    "\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"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\"\n",
    "Tavily = TavilySearchAPIRetriever(k=5,include_images = True,include_raw_content = True,search_depth = 'advanced')\n",
    "#print(Tavily.invoke(\"What is Langraph ?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "\n",
    "Python_Interp = PythonREPL()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLMs :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coder LLM:\n",
    "\n",
    "#from typing import Literal\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "class Coder_Output(BaseModel):\n",
    "    Code: str = Field(...,description=\"This is an optimized ,accurate  and Correct PYthon Code\")\n",
    "\n",
    "\n",
    "# LLM with Structered Output\n",
    "llm = ChatOpenAI(api_key=\"ollama\",model=\"llama3.1:8b-instruct-fp16\",base_url=\"http://localhost:11434/v1\")\n",
    "structured_llm_coder = llm.with_structured_output(Coder_Output)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are an expert at generating python Code to response a User Question.\\n\n",
    "Your Code Should be Complete with all used libraries.\\n\n",
    "Your Code should be correct and Optimized as possible.\\n\n",
    "Give just the Code Don't add any additional details or Explainations.\"\"\"\n",
    "\n",
    "coder_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"The User question here :\\n {question}\"),\n",
    "    ]\n",
    ")\n",
    "  \n",
    "Coder = coder_prompt | structured_llm_coder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Question = \"Code To desplay a matplotlib graph of cosin math function ?\"\n",
    "input = {\"question\":Question}\n",
    "print(Coder.invoke(input).Code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Researcher :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Researcher LLM:\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# LLM with Structered Output\n",
    "llm = ChatOpenAI(api_key=\"ollama\",model=\"llama3.1:8b-instruct-fp16\",base_url=\"http://localhost:11434/v1\")\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are an expert at using web Search content to respond a User question.\\n\n",
    "Your Response Should be optimazed concise and Clear.\\n\n",
    "Respond just to the User Question Don't add any details or Explanaition, keep the reponse concise.\"\"\"\n",
    "\n",
    "Resercher_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"The User question here :\\n {question}\\n\\n Web Search content here : \\n {WebResults}\"),\n",
    "    ]\n",
    ")\n",
    "  \n",
    "Resercher = Resercher_prompt | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Question = \"Remplissage Barrage Alwahda aujourd'hui ? \"\n",
    "Results = Tavily.invoke(Question)\n",
    "Context = \"\\n\\n-----\\n\\n\".join(result.page_content for result in Results)\n",
    "input = {\"question\":Question,\"WebResults\":Context}\n",
    "print(Context)\n",
    "print(\"\\n\\n-----------------------------------------------\\n\\n\")\n",
    "print(Resercher.invoke(input).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Superviser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "class Superviser_Output(BaseModel):\n",
    "    next: Literal[\"Coder\", \"Researcher\", \"FINISH\"] = Field(..., description=\"This is the name of the next worker\")\n",
    "\n",
    "\n",
    "# LLM with Structured Output\n",
    "llm = ChatOpenAI(api_key=\"ollama\", model=\"mistral-nemo:12b-instruct-2407-fp16\", base_url=\"http://localhost:11434/v1\")\n",
    "structured_llm_Superviser = llm.with_structured_output(Superviser_Output)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are a supervisor tasked with managing a conversation between the\n",
    "    following workers:\\n\\n \n",
    "        Researcher : A worker that Search in the net.\n",
    "        Coder      : A worker the generate code.\n",
    "        FINISH     : key word To finish the execution.\n",
    "        \\n\\n\n",
    "    Given a user request respond with the Name of worker to act next. \\n\n",
    "    Each worker will perform a task and respond with his results. \\n\n",
    "    Your Response should be just the Name of the worker from here [Researcher,Coder,FINISH] No additional details or Explanations.\\n\n",
    "    When finished, respond with FINISH.\"\"\"\n",
    "\n",
    "\n",
    "Superviser_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Given the conversation above, who should act next?\"\n",
    "            \" Or should we FINISH? Select one of: {options}\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "options = [\"Researcher\", \"Coder\",\"FINISH\"]\n",
    "Superviser = Superviser_prompt | structured_llm_Superviser\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize The Graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Sequence, TypedDict,Annotated\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langchain_core.messages import BaseMessage, HumanMessage,ToolMessage,AIMessage\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    next:str\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(State)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nodes Functions :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Colors import BLUE,RED,YELLOW,GREEN,RESET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLMs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Resercher_function(state:State):\n",
    "    print(\"State messages :\",state[\"messages\"])\n",
    "    Question = state[\"messages\"][0]\n",
    "    Results = Tavily.invoke(Question)\n",
    "    Context = \"\\n\\n-----\\n\\n\".join(result.page_content for result in Results)\n",
    "    input = {\"question\":Question,\"WebResults\":Context}\n",
    "    print(f\"üí°{GREEN}Resercher is Thinking !{RESET}\")\n",
    "    report = Resercher.invoke(input)\n",
    "    return {\"messages\":report}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Coder_function(state:State):\n",
    "    print(\"State messages :\",state[\"messages\"])\n",
    "    Question = state[\"messages\"][0]\n",
    "    input = {\"question\":Question}\n",
    "    print(f\"üí°{GREEN}Coder is Thinking !{RESET}\")\n",
    "    code = Coder.invoke(input)\n",
    "    return {\"messages\":report}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Num_Trials=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Superviser_function(state:State):\n",
    "    print(\"State messages :\",state[\"messages\"])\n",
    "    conversation = state[\"messages\"]\n",
    "    input = {\"messages\":conversation,\"options\":options}\n",
    "    print(f\"üí°{GREEN}Superviser is Thinking !{RESET}\")\n",
    "    for i in range(Num_Trials):\n",
    "        print(f\"üí°{GREEN}Superviser {i} is Thinking !{RESET}\")\n",
    "        response = Superviser.invoke(input)\n",
    "        if response:\n",
    "            return {\"messages\":response,\"next\":response.next}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Choose(state:State):\n",
    "    return state[\"next\"]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design the Graph:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder.add_node(\"Researcher\",Resercher_function)\n",
    "graph_builder.add_node(\"Coder\",Coder_function)\n",
    "graph_builder.add_node(\"Superviser\",Superviser_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder.add_edge(START,\"Superviser\")\n",
    "graph_builder.add_edge(\"Coder\",\"Superviser\")\n",
    "graph_builder.add_edge(\"Researcher\",\"Superviser\")\n",
    "graph_builder.add_conditional_edges(\"Superviser\",Choose,{\"Coder\":\"Coder\", \"Researcher\":\"Researcher\", \"FINISH\":\"__end__\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph(xray= True).draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interact with The Graph :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "My_Quetion = \"What is a node in Langraph?\"\n",
    "\n",
    "for event in graph.stream({\"messages\": [(\"user\", My_Quetion)]}):\n",
    "    for value in event.values():\n",
    "        print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
