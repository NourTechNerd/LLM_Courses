{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instal Ollama :\n",
    "\n",
    "curl -fsSL https://ollama.com/install.sh | sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSI escape codes\n",
    "RED = \"\\033[91m\"\n",
    "GREEN = \"\\033[92m\"\n",
    "YELLOW = \"\\033[93m\"\n",
    "BLUE = \"\\033[94m\"\n",
    "RESET = \"\\033[0m\"\n",
    "\n",
    "print(f\"{RED}This is red text{RESET}\")\n",
    "print(f\"{GREEN}This is green text{RESET}\")\n",
    "print(f\"{YELLOW}This is yellow text{RESET}\")\n",
    "print(f\"{BLUE}This is blue text{RESET}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part1 : Graph with a single Node "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "#os.environ[\"LANGCHAIN_PROJECT\"] = \"LangGraph Tutorial\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llm = Ollama(model=\"qwen2:1.5b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot(state):\n",
    "    prompt = state[\"messages\"]\n",
    "    response = llm.invoke(prompt)\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- each time the node is executed the function is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder.add_node(\"chatbot\", chatbot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "graph_builder.add_edge(\"chatbot\",END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Our_Graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(Our_Graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    user_input = input(\"User: \")\n",
    "    if user_input.lower() in [\"q\"]:\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "    for event in Our_Graph.stream({\"messages\": (\"user\", user_input)}):\n",
    "        for value in event.values():\n",
    "            print(\"Assistant:\", value[\"messages\"][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part2 : Graph with 2 nodes (LLM + Search Tool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the Graph and the State :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages import BaseMessage, HumanMessage,ToolMessage,AIMessage\n",
    "from langgraph.graph import StateGraph, START\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "# Create the Object that we will use to bulid our graph\n",
    "graph_builder = StateGraph(State)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tavily node:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tavily is the first research engine optimized for AI agents and LLMs.\n",
    "- Main key features of this API is :\n",
    "   - The results are page summaries our chat bot can use to answer questions not the just a dummy Scraping (Amazing !!).\n",
    "   - The API choose the best results from search.\n",
    "\n",
    "[Get Your Tavily API Key](https://app.tavily.com/home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"TAVILY_API_KEY\"] = getpass.getpass()\n",
    "#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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "Tavily = TavilySearchResults(max_results=2)\n",
    "tools = [Tavily]\n",
    "#tool.invoke(\"What's a 'node' in LangGraph?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_core.messages import ToolMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_AIMessage(state:State):\n",
    "    messages = state.get(\"messages\", []) \n",
    "    # .get : Return the value associeted to the key \"messages\" if the key not exist it returns an empty list\n",
    "    if messages:\n",
    "        message = messages[-1]\n",
    "        # message is the last in messages list.\n",
    "    return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tavily_Function(state:State):\n",
    "    message = Get_AIMessage(state)\n",
    "        \n",
    "    Tavily_call = message.tool_calls[0]\n",
    "    # Execute the Tool called by the LLM\n",
    "    tool_arguments = Tavily_call[\"args\"]\n",
    "    \n",
    "    print(f\"âš™ï¸{YELLOW} Tavily is Executing{RESET}\")\n",
    "    \n",
    "    Tavily_result = Tavily.invoke(tool_arguments)\n",
    "    Tavily_Message =  ToolMessage(content=json.dumps(Tavily_result),name=Tavily_call[\"name\"],tool_call_id=Tavily_call[\"id\"])\n",
    "    \n",
    "    return {\"messages\": Tavily_Message}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder.add_node(\"Tavily\", Tavily_Function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM node:\n",
    "\n",
    "- **bind_Tools** : allow us to pass the Tools Schemas (descriptions) with the prompt every Time we send a quetion to the LLM. So the LLM can request a tool(s) if he need it (them).\n",
    "- When you just use bind_tools(tools), the model can choose whether to return one tool call, multiple tool calls, or no tool calls at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "\n",
    "llm = OllamaFunctions(model=\"phi3:3.8b\",format= \"json\")\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot_Function(state: State):\n",
    "    print(\"State messages :\",state[\"messages\"])\n",
    "    print(f\"ðŸ’¡{GREEN}LLM is Thinking !{RESET}\")\n",
    "    \n",
    "    prompt = state[\"messages\"]\n",
    "    response = llm_with_tools.invoke(prompt)\n",
    "    return {\"messages\": response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder.add_node(\"LLM\", chatbot_Function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design the Graph :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "\n",
    "def Condition_Function(state: State):\n",
    "    \"\"\"\n",
    "    Use in the conditional_edge to route to the ToolsNode if the last message\n",
    "    has tool calls. Otherwise, route to the end.\n",
    "    \"\"\"\n",
    "    messages = state.get(\"messages\", [])\n",
    "    \n",
    "    if isinstance(state, list):\n",
    "        ai_message = state[-1]\n",
    "        \n",
    "    elif messages : # check that messages is not an empty list\n",
    "        ai_message = messages[-1]\n",
    "    else:\n",
    "        raise ValueError(f\"No messages found in input state to tool_edge: {state}\")\n",
    "    if hasattr(ai_message, \"tool_calls\") and len(ai_message.tool_calls) > 0: # Check if the LLM ask for a tool\n",
    "        \n",
    "        if Ask_for_Tavily(ai_message.tool_calls):\n",
    "            return \"Ask for Tavily\"\n",
    "    return \"Ask for END\"\n",
    "\n",
    "def Ask_for_Tavily(tool_calls:list):\n",
    "    for tool_call in tool_calls:\n",
    "        if tool_call['name'] == \"tavily_search_results_json\":\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder.add_conditional_edges(\"LLM\",Condition_Function,{\"Ask for Tavily\": \"Tavily\", \"Ask for END\": \"__end__\"},)\n",
    "graph_builder.add_edge(\"Tavily\", \"LLM\")\n",
    "graph_builder.add_edge(START, \"LLM\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interact With The Graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "My_Quetion = \"What is a node in Langraph?\"\n",
    "\n",
    "for event in graph.stream({\"messages\": [(\"user\", My_Quetion)]}):\n",
    "    for value in event.values():\n",
    "        print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part3 :Graph with Multiple Nodes As I want :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph initialization :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph, START\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "# Create the Object that we will use to bulid our graph\n",
    "graph_builder = StateGraph(State)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools Nodes :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weather Tool node :\n",
    "- We will a Weather API built in by langchain.\n",
    "- I already Construct the tool in My_Tools.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from My_Tools import OpenWeather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test the tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenWeather\n",
      "Give the Weather information for a specific Location\n",
      "{'Location': {'title': 'Location', 'description': 'The Location is often a city name like London, Casablanca ...', 'type': 'string'}}\n"
     ]
    }
   ],
   "source": [
    "print(OpenWeather.name)\n",
    "print(OpenWeather.description)\n",
    "print(OpenWeather.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Meknes, the current weather is as follows:\n",
      "Detailed status: broken clouds\n",
      "Wind speed: 4.12 m/s, direction: 330Â°\n",
      "Humidity: 44%\n",
      "Temperature: \n",
      "  - Current: 37.49Â°C\n",
      "  - High: 37.49Â°C\n",
      "  - Low: 37.49Â°C\n",
      "  - Feels like: 44.08Â°C\n",
      "Rain: {}\n",
      "Heat index: None\n",
      "Cloud cover: 75%\n"
     ]
    }
   ],
   "source": [
    "print(OpenWeather.invoke({\"Location\": \"Meknes\"})) # invoke execute the run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create the Node Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import ToolMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_AIMessage(state:State):\n",
    "    messages = state.get(\"messages\", []) \n",
    "    # .get : Return the value associeted to the key \"messages\" if the key not exist it returns an empty list\n",
    "    if messages:\n",
    "        message = messages[-1]\n",
    "        # message is the last in messages list.\n",
    "    return message\n",
    "\n",
    "def OpenWeather_Function(state:State,tool = OpenWeather):\n",
    "    message = Get_AIMessage(state)\n",
    "        \n",
    "    Tool_call = message.tool_calls[0]\n",
    "    # Execute the Tool called by the LLM\n",
    "    Tool_arguments = Tool_call[\"args\"]\n",
    "    Tool_Name = Tool_call[\"name\"]\n",
    "    print(f\"âš™ï¸{YELLOW} {Tool_Name} is Executing{RESET}\")\n",
    "    \n",
    "    Tool_result = tool.invoke(Tool_arguments)\n",
    "    Tool_Message =  ToolMessage(content=Tool_result,name=Tool_call[\"name\"],tool_call_id=Tool_call[\"id\"])\n",
    "    \n",
    "    return {\"messages\": Tool_Message}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder.add_node(\"OpenWeather\", OpenWeather_Function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tavily Tool Node :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from My_Tools import Tavily\n",
    "from langchain_core.messages import ToolMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tavily\n",
      "A search engine optimized for comprehensive, accurate, and trusted results. Useful for when you need to answer questions about current events. Input should be a search query.\n",
      "{'query': {'title': 'Query', 'description': 'search query to look up', 'type': 'string'}}\n"
     ]
    }
   ],
   "source": [
    "print(Tavily.name)\n",
    "print(Tavily.description)\n",
    "print(Tavily.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_AIMessage(state:State):\n",
    "    messages = state.get(\"messages\", []) \n",
    "    # .get : Return the value associeted to the key \"messages\" if the key not exist it returns an empty list\n",
    "    if messages:\n",
    "        message = messages[-1]\n",
    "        # message is the last in messages list.\n",
    "    return message\n",
    "\n",
    "def Tavily_Function(state:State,tool = Tavily):\n",
    "    message = Get_AIMessage(state)\n",
    "        \n",
    "    Tool_call = message.tool_calls[0]\n",
    "    # Execute the Tool called by the LLM\n",
    "    Tool_arguments = Tool_call[\"args\"]\n",
    "    Tool_Name = Tool_call[\"name\"]\n",
    "    print(f\"âš™ï¸{YELLOW} {Tool_Name} is Executing{RESET}\")\n",
    "    \n",
    "    Tool_result = tool.invoke(Tool_arguments)\n",
    "    Tool_Message =  ToolMessage(content=Tool_result,name=Tool_call[\"name\"],tool_call_id=Tool_call[\"id\"])\n",
    "    \n",
    "    return {\"messages\": Tool_Message}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder.add_node(\"Tavily\", Tavily_Function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM node :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [OpenWeather,Tavily]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "\n",
    "llm = OllamaFunctions(model=\"gemma2:9b\",format= \"json\")\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LLM_function(state:State):\n",
    "    print(f\"{BLUE}State messages :{RESET}\",state[\"messages\"])\n",
    "    print(f\"ðŸ’¡{GREEN}LLM is Thinking !{RESET}\")\n",
    "    \n",
    "    prompt = state[\"messages\"]\n",
    "    response = llm_with_tools.invoke(prompt)\n",
    "    return {\"messages\": response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder.add_node(\"LLM\", LLM_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desing the Graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Condition_Function(state: State):\n",
    "    messages = state.get(\"messages\", [])\n",
    "    \n",
    "    if isinstance(state, list):\n",
    "        ai_message = state[-1]\n",
    "        \n",
    "    elif messages : # check that messages is not an empty list\n",
    "        ai_message = messages[-1]\n",
    "    else:\n",
    "        raise ValueError(f\"No messages found in State: {state}\")\n",
    "    if hasattr(ai_message, \"tool_calls\") and len(ai_message.tool_calls) > 0: # Check if the LLM ask for a tool\n",
    "        \n",
    "        for tool_call in ai_message.tool_calls:\n",
    "            if tool_call['name'] == \"OpenWeather\":\n",
    "                return \"OpenWeather\"\n",
    "            if tool_call['name'] == \"Tavily\":\n",
    "                return \"Tavily\"\n",
    "    return \"__end__\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder.add_conditional_edges(\"LLM\",Condition_Function)\n",
    "graph_builder.add_edge(\"OpenWeather\", \"LLM\") # if the OpenWeather is invoked so the next node will be the LLM\n",
    "graph_builder.add_edge(\"Tavily\", \"LLM\")\n",
    "graph_builder.add_edge(START, \"LLM\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interact With The Graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "My_Quetion = \"What is a dog ?\"\n",
    "Input_Message = {\"messages\": [(\"human\", My_Quetion)]}\n",
    "\n",
    "async for chunk in graph.astream(Input_Message, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 : Add Memory to our Agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "\n",
    "memory = SqliteSaver.from_conn_string(\":memory:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llm = Ollama(model=\"gemma2:9b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LLM_function(state:State):\n",
    "    print(f\"{BLUE}State messages :{RESET}\",state[\"messages\"])\n",
    "    print(f\"ðŸ’¡{GREEN}LLM is Thinking !{RESET}\")\n",
    "    \n",
    "    prompt = state[\"messages\"]\n",
    "    response = llm.invoke(prompt)\n",
    "    return {\"messages\": response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder.add_node(\"LLM\", LLM_function)\n",
    "graph_builder.add_edge(START, \"LLM\")\n",
    "graph_builder.add_edge(\"LLM\",END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = graph_builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}} \n",
    "# We configurate here the id of the State to load by the graph\n",
    "\n",
    "My_Quetion = \"Hi Do you know About Morroco?\"\n",
    "Input_Message = {\"messages\": [(\"user\", My_Quetion)]}\n",
    "\n",
    "# The config is the **second positional argument** to stream() or invoke()!\n",
    "events = graph.stream(Input_Message, config, stream_mode=\"values\")\n",
    "for event in events:\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}} \n",
    "# We configurate here the id of the State to load by the graph\n",
    "\n",
    "My_Quetion = \"What is the Capital?\"\n",
    "Input_Message = {\"messages\": [(\"user\", My_Quetion)]}\n",
    "# The config is the **second positional argument** to stream() or invoke()!\n",
    "\n",
    "events = graph.stream(Input_Message, config, stream_mode=\"values\")\n",
    "for event in events:\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "State_tread1 =graph.get_state(config)\n",
    "print(State_tread1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5 : Human in the Loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "\n",
    "memory = SqliteSaver.from_conn_string(\":memory:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(State)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AskNews Tool Node :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from My_Tools import AskNews\n",
    "from langchain_core.messages import ToolMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(AskNews.name) \n",
    "# This is importatnt because This is the Name that the LLM will use to call the Tool \n",
    "print(AskNews.description)\n",
    "print(AskNews.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_AIMessage(state:State):\n",
    "    messages = state.get(\"messages\", []) \n",
    "    # .get : Return the value associeted to the key \"messages\" if the key not exist it returns an empty list\n",
    "    if messages:\n",
    "        message = messages[-1]\n",
    "        # message is the last in messages list.\n",
    "    return message\n",
    "\n",
    "def AskNews_Function(state:State,tool = AskNews):\n",
    "    message = Get_AIMessage(state)\n",
    "        \n",
    "    Tool_call = message.tool_calls[0]\n",
    "    # Execute the Tool called by the LLM\n",
    "    Tool_arguments = Tool_call[\"args\"]\n",
    "    Tool_Name = Tool_call[\"name\"]\n",
    "    print(f\"âš™ï¸{YELLOW} {Tool_Name} is Executing{RESET}\")\n",
    "    \n",
    "    Tool_result = tool.invoke(Tool_arguments)\n",
    "    Tool_Message =  ToolMessage(content=Tool_result,name=Tool_call[\"name\"],tool_call_id=Tool_call[\"id\"])\n",
    "    \n",
    "    return {\"messages\": Tool_Message}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder.add_node(\"AskNews\", AskNews_Function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM node :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [AskNews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "\n",
    "llm = OllamaFunctions(model=\"gemma2:9b\",format= \"json\")\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LLM_function(state:State):\n",
    "    print(f\"{BLUE}State messages :{RESET}\",state[\"messages\"])\n",
    "    print(f\"ðŸ’¡{GREEN}LLM is Thinking !{RESET}\")\n",
    "    \n",
    "    prompt = state[\"messages\"]\n",
    "    response = llm_with_tools.invoke(prompt)\n",
    "    return {\"messages\": response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder.add_node(\"LLM\", LLM_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desing the Graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Condition_Function(state: State):\n",
    "    messages = state.get(\"messages\", [])\n",
    "    \n",
    "    if isinstance(state, list):\n",
    "        ai_message = state[-1]\n",
    "        \n",
    "    elif messages : # check that messages is not an empty list\n",
    "        ai_message = messages[-1]\n",
    "    else:\n",
    "        raise ValueError(f\"No messages found in State: {state}\")\n",
    "    if hasattr(ai_message, \"tool_calls\") and len(ai_message.tool_calls) > 0: # Check if the LLM ask for a tool\n",
    "        \n",
    "        for tool_call in ai_message.tool_calls:\n",
    "            if tool_call['name'] == \"asknews_search\":\n",
    "                return \"AskNews\"\n",
    "    return \"__end__\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder.add_conditional_edges(\"LLM\",Condition_Function)\n",
    "graph_builder.add_edge(\"AskNews\", \"LLM\")\n",
    "graph_builder.add_edge(START, \"LLM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = graph_builder.compile(\n",
    "    checkpointer=memory,\n",
    "    # This is new!\n",
    "    #interrupt_before=[\"LLM\"],\n",
    "    # We will Stop the graph after LLM node is executed\n",
    "    interrupt_after=[\"LLM\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interact With The Graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"What is the effect of Global warming in Morroco in 2024?\"\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "# The config is the **second positional argument** to stream() or invoke()!\n",
    "events = graph.stream(\n",
    "    {\"messages\": [(\"user\", user_input)]}, config, stream_mode=\"values\"\n",
    ")\n",
    "for event in events:\n",
    "    if \"messages\" in event:\n",
    "        event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see thet the Graph Stop After the LLM node has been executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `None` will append nothing new to the current state, letting it resume as if it had never been interrupted\n",
    "events = graph.stream(None, config, stream_mode=\"values\")\n",
    "for event in events:\n",
    "    if \"messages\" in event:\n",
    "        event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6 : Modify the Sate Manualy :\n",
    "- Until to Now the graph update the Sate with New messages automaticly *(Of course the Return of every Node-function's return should be a Dic with key \"messages\")*\n",
    "- Now we Can add messages to the State, or Overite messages already exist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Messages :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Graphs import graph2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "I'm learning LangGraph. Could you do some research on it for me?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  tavily_search_results_json (call_f1niu2ia)\n",
      " Call ID: call_f1niu2ia\n",
      "  Args:\n",
      "    query: LangGraph\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search_results_json\n",
      "\n",
      "[{\"url\": \"https://www.langchain.com/langgraph\", \"content\": \"LangGraph sets the foundation for how we can build and scale AI workloads \\u2014 from conversational agents, complex task automation, to custom LLM-backed experiences that 'just work'. The next chapter in building complex production-ready features with LLMs is agentic, and with LangGraph and LangSmith, LangChain delivers an out-of-the-box solution ...\"}, {\"url\": \"https://github.com/langchain-ai/langgraph\", \"content\": \"LangGraph is a framework for creating stateful, multi-actor applications with LLMs, using cycles, controllability, and persistence. It integrates with LangChain and LangSmith, and supports streaming, human-in-the-loop, and memory features.\"}]\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      " LangGraph is a framework developed by LangChain for creating stateful, multi-actor applications using Large Language Models (LLMs). It allows the building of complex AI workloads such as conversational agents, task automation, and custom experiences backed by LLMs. The framework supports features like streaming, human-in-the-loop, memory management, cycles, controllability, and persistence. LangGraph can be integrated with other tools from LangChain and LangSmith, and it sets the foundation for building agentic applications using LLMs, making them easier to scale and maintain.\n",
      "\n",
      "Reference(s):\n",
      "* [LangChain Website](https://www.langchain.com/langgraph)\n",
      "* [LangGraph on Github](https://github.com/langchain-ai/langgraph)\n"
     ]
    }
   ],
   "source": [
    "user_input = \"I'm learning LangGraph. Could you do some research on it for me?\"\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "# The config is the **second positional argument** to stream() or invoke()!\n",
    "events = graph2.stream({\"messages\": [(\"user\", user_input)]}, config,stream_mode=\"values\")\n",
    "for event in events:\n",
    "    if \"messages\" in event:\n",
    "        event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Graph_execution = graph1.get_state(config)\n",
    "Graph_execution.values[\"messages\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Because We interupt the graph , *Graph_execution.next* attribute has value as the next nodes names that will be executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Graph_execution.next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can add ToolMessage and AIMessage to the State like as the \"tools\" an \"chatbot\" nodes has been executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage,ToolMessage\n",
    "\n",
    "answer = (\n",
    "    \"LangGraph is a library for building stateful, multi-actor applications with LLMs.\"\n",
    ")\n",
    "new_messages = [\n",
    "    ToolMessage(content=answer, tool_call_id=\"1111111\"),\n",
    "    AIMessage(content=answer),\n",
    "]\n",
    "graph1.update_state(config,{\"messages\": new_messages})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lets Cheack if the State updated or Not ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Graph_execution = graph1.get_state(config)\n",
    "Graph_execution.values[\"messages\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The graph will end execution because the AiMessge dosn't countain a tool_calls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Graph_execution.next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change Messages :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"What happen to Trump in the Last Days?\"\n",
    "config = {\"configurable\": {\"thread_id\": \"3\"}} \n",
    "events = graph2.stream(\n",
    "    {\"messages\": [(\"user\", user_input)]}, config, stream_mode=\"values\"\n",
    ")\n",
    "for event in events:\n",
    "    if \"messages\" in event:\n",
    "        event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "snapshot = graph1.get_state(config)\n",
    "existing_message = snapshot.values[\"messages\"][-1]\n",
    "print(\"Original\")\n",
    "print(\"Message ID\", existing_message.id)\n",
    "print(existing_message.tool_calls[0])\n",
    "new_tool_call = existing_message.tool_calls[0].copy()\n",
    "new_tool_call[\"args\"][\"query\"] = \"Trumpy Attack News BBC\"\n",
    "new_message = AIMessage(\n",
    "    content=existing_message.content,\n",
    "    tool_calls=[new_tool_call],\n",
    "    # Important! The ID is how LangGraph knows to REPLACE the message in the state rather than APPEND this messages\n",
    "    id=existing_message.id,\n",
    ")\n",
    "\n",
    "print(\"Updated\")\n",
    "print(new_message.tool_calls[0])\n",
    "print(\"Message ID\", new_message.id)\n",
    "graph1.update_state(config, {\"messages\": [new_message]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Graph_execution = graph1.get_state(config)\n",
    "Graph_execution.values[\"messages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = graph1.stream(None, config, stream_mode=\"values\")\n",
    "for event in events:\n",
    "    if \"messages\" in event:\n",
    "        event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM_Courses",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
