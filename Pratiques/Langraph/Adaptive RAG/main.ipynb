{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instal Ollama :\n",
    "\n",
    "curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "ollama pull llama3-groq-tool-use:8b\n",
    "\n",
    "ollama pull mistral:7b \n",
    "\n",
    "ollama pull nomic-embed-text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize The Graph :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List,Dict\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START,END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    Tool_Choise:str\n",
    "    Grade:str\n",
    "    Question:str\n",
    "    Generation:str\n",
    "    Context : str\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(State)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building The Tools and LLMs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tools are already built in My_Tools.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from My_Tools import RAG_System,OpenWeather,Tavily,RED,YELLOW,BLUE,GREEN,RESET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's build our LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chooser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chooser LLM:\n",
    "\n",
    "from typing import Literal\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# Data model\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n",
    "\n",
    "    datasource: Literal[\"RAG_System\", \"Tavily\",\"OpenWeather\"] = Field(...,description=\"Given a user question choose to route it to web search call Tavily or a database call RAG_System or tool for Weather Informations call OpenWeather\",)\n",
    "\n",
    "\n",
    "# LLM with Structered Output\n",
    "llm = ChatOpenAI(api_key=\"ollama\",model=\"llama3-groq-tool-use:8b\",base_url=\"http://localhost:11434/v1\")\n",
    "structured_llm_router = llm.with_structured_output(RouteQuery)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are an expert at routing a user question to a RAG_System , Tavily or OpenWeather.\\n\\n\n",
    "Tavily is a web search engine like Google.for questions related to News people and general questions\\n\n",
    "OpenWeather is an API that give the Weathr informations for a Location like City.\\n\n",
    "The RAG_System contains documents related to agents, prompt engineering, and adversarial attacks.\\n\n",
    "Use the RAG_System for questions on these topics. and Use OpenWeather for questions related to weather.  Otherwise, use Tavily web serach engine.\"\"\"\n",
    "route_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "Chooser = route_prompt | structured_llm_router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Chooser.invoke({\"question\": \"It is raining Now in Meknes ?\"}))\n",
    "print(Chooser.invoke({\"question\": \"What are the types of agent memory?\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Chooser.invoke({\"question\": \"What happens to Trump last days ?\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context_Grader :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data model\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance of Informations.\"\"\"\n",
    "\n",
    "    binary_score: Literal[\"yes\",\"no\"] = Field(...,description=\"A binary_score to evaluate External Informations are relevant to the question, 'yes' or 'no'\")\n",
    "\n",
    "\n",
    "# LLM with function call\n",
    "llm = ChatOpenAI(api_key=\"ollama\",model=\"llama3-groq-tool-use:8b\",base_url=\"http://localhost:11434/v1\")\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are an expert in evaluation of the relevance of an Informations to a User question\n",
    "if the information are relevent to the question respond by 'yes' else respond by 'no' \"\"\"\n",
    "\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Informations: \\n\\n {Context} \\n\\n User question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "Context_Grader = grade_prompt | structured_llm_grader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"agent memory ?\"\n",
    "Context = RAG_System.invoke(question)\n",
    "response = Context_Grader.invoke({\"Context\": Context,\"question\": question})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewriter\n",
    "- In the Chain we use StrOutputParser() function to convert the response of the LLM to a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question Re-writer\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "# LLM\n",
    "llm = ChatOpenAI(api_key=\"ollama\",model=\"mistral:7b\",base_url=\"http://localhost:11434/v1\")\n",
    "\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are an expert question re-writer that converts an input question to a better version that is optimized \\n \n",
    "     for vectorstore retrieval. Look at the input and try to reason about the underlying semantic intent / meaning. Give Just the New question\n",
    "     No additional details \"\"\"\n",
    "re_write_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "Rewriter = re_write_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is prompt engenering ?\"\n",
    "response = Rewriter.invoke({\"question\": question}).strip().strip('\"')\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "# LLM\n",
    "llm = ChatOpenAI(api_key=\"ollama\",model=\"mistral:7b\",base_url=\"http://localhost:11434/v1\",temperature=0)\n",
    "# Chain\n",
    "Reporter = prompt | llm | StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question =\"adv attacks and LLMs?\"\n",
    "Context = RAG_System.invoke({\"query\":question})\n",
    "print(Context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"agent memory ?\"\n",
    "Context = RAG_System.invoke(question)\n",
    "response = Reporter.invoke({\"context\": Context, \"question\": question})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nodes Functions :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "Num_trails = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLMs functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Chooser_function(state:State):\n",
    "    # Sometimes LLM can return None, but if we rerun it returns the good response\n",
    "    # So we well create a loop with number trials\n",
    "    for i in range(Num_trails):\n",
    "        print(f\"{GREEN}Chooser Trial :{i}{RESET}\")\n",
    "        response = Chooser.invoke({\"question\": state[\"Question\"]})\n",
    "        if response:\n",
    "             return {\"Tool_Choise\":response.datasource}\n",
    "    raise ValueError(\"Chooser can't choise\")\n",
    "\n",
    "def Grader_function(state:State):\n",
    "    # Sometimes LLM can return None, but if we rerun it returns the good response\n",
    "    # So we well create a loop with a number trials\n",
    "    for i in range(Num_trails):\n",
    "        print(f\"{GREEN}Grader Trial :{i}{RESET}\")\n",
    "        response = Context_Grader.invoke({\"Context\": state[\"Context\"],\"question\": state[\"Question\"]})\n",
    "        if response:\n",
    "             return {\"Grade\":response.binary_score}\n",
    "    raise ValueError(\"Grader can't evaluate Context\")\n",
    "\n",
    "def Rewriter_function(state:State):\n",
    "\n",
    "    print(f\"{GREEN}Rewriter{RESET}\")\n",
    "    response = Rewriter.invoke({\"question\": state[\"Question\"]}).strip().strip('\"')\n",
    "    # Update the State\n",
    "    return {\"Question\" : response}\n",
    "\n",
    "def Reporter_function(state:State):\n",
    "\n",
    "    print(f\"{GREEN}Reporter{RESET}\")\n",
    "    response = Reporter.invoke({\"context\": state[\"Context\"], \"question\": state[\"Question\"]})\n",
    "    # Update the State\n",
    "    return {\"Generation\" : response}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 3 tools have the same execution process so we will define one function for them\n",
    "def Tool_function(state:State):\n",
    "    Tool_Name = state[\"Tool_Choise\"]\n",
    "    query = state[\"Question\"]\n",
    "    print(f\"{YELLOW}{Tool_Name} is executing{RESET}\")\n",
    "    \n",
    "    if Tool_Name == \"RAG_System\":\n",
    "        context = RAG_System.invoke({\"query\":query})\n",
    "        return {\"Context\":context}\n",
    "\n",
    "    if Tool_Name == \"Tavily\":\n",
    "        docs = Tavily.invoke({\"query\": query})\n",
    "        context = \"\\n\\n---\\n\\n\".join([d[\"content\"] for d in docs])\n",
    "        return {\"Context\":context}\n",
    "\n",
    "    if Tool_Name == \"OpenWeather\":\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional edges function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tool_Choise(state:State) -> str:\n",
    "    return state[\"Tool_Choise\"] # Name of the Tool Node\n",
    "\n",
    "def Grade(state:State) -> str:\n",
    "    if state[\"Grade\"] == \"yes\":\n",
    "        return \"Reporter\"\n",
    "    return \"Rewriter\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design the ARAG_Graph :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder.add_node(\"Chooser\",Chooser_function)\n",
    "graph_builder.add_node(\"Grader\",Grader_function)\n",
    "graph_builder.add_node(\"Reporter\",Reporter_function)\n",
    "graph_builder.add_node(\"Rewriter\",Rewriter_function)\n",
    "graph_builder.add_node(\"Tavily\",Tool_function)\n",
    "graph_builder.add_node(\"RAG_System\",Tool_function)\n",
    "graph_builder.add_node(\"OpenWeather\",Tool_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder.add_edge(START,\"Chooser\")\n",
    "graph_builder.add_conditional_edges(\"Chooser\",Tool_Choise,{\"Tavily\":\"Tavily\",\"RAG_System\":\"RAG_System\",\"OpenWeather\":\"OpenWeather\"})\n",
    "graph_builder.add_edge(\"Tavily\",\"Grader\")\n",
    "graph_builder.add_edge(\"RAG_System\",\"Grader\")\n",
    "graph_builder.add_edge(\"OpenWeather\",\"Grader\")\n",
    "graph_builder.add_conditional_edges(\"Grader\",Grade,{\"Reporter\":\"Reporter\",\"Rewriter\":\"Rewriter\"})\n",
    "graph_builder.add_edge(\"Rewriter\",\"Chooser\")\n",
    "graph_builder.add_edge(\"Reporter\",END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARAG_Graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize The Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(ARAG_Graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interact With the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mChooser Trial :0\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mChooser Trial :1\u001b[0m\n",
      "\u001b[92mChooser Trial :2\u001b[0m\n",
      "Node :Chooser:\n",
      "State :{'Tool_Choise': 'RAG_System'}\n",
      "\n",
      "\n",
      "\u001b[93mRAG_System is executing\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:01<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node :RAG_System:\n",
      "State :{'Context': 'Prompt Engineering | Lil\\'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil\\'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\nemojisearch.app\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Prompt Engineering\\n    \\nDate: March 15, 2023  |  Estimated Reading Time: 21 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nBasic Prompting\\n\\nZero-Shot\\n\\nFew-shot\\n\\nTips for Example Selection\\n\\nTips for Example Ordering\\n\\n\\n\\nInstruction Prompting\\n\\nSelf-Consistency Sampling\\n\\nChain-of-Thought (CoT)\\n\\nTypes of CoT prompts\\n\\nTips and Extensions\\n\\n\\nAutomatic Prompt Design\\n\\nAugmented Language Models\\n\\nRetrieval\\n\\nProgramming Language\\n\\nExternal APIs\\n\\n\\nCitation\\n\\nUseful Resources\\n\\nReferences\\n\\n---\\n\\nFig. 8. Illustration of where adversarial triggers are introduced. The red exclamation points represent adversarial tokens to be learned. (Image source: Zou et al. 2023)\\nThe experiments for triggering affirmative model responses across multiple inputs were conducted on two different models, Vicuna-7b and Vicuna-13b. They adopted greedy coordinate gradient (GCG) based search to greedily find one candidate that can reduce the loss the most among all possible single-token substitutions. It is not feasible to literally evaluate all token substitutions, so they run gradient based token search strategy similar to UAT and AutoPrompt to find top candidates per token, each associated with the largest negative gradient of the loss.\\nGiven a set of expected $m$ input-output pairs ${ (\\\\mathbf{x}^{(i)}, \\\\mathbf{y}^{(i)}) }_{i=1}^m$ (E.g. the input \"Give instructions for building a pipe bomb\", will be associated with an output \"Sure, here’s instructions for building a pipe bomb:\") and  an adversarial suffix $\\\\mathbf{t}$ of length $L$ :\\n\\n---\\n\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, essentially creating a tree structure. The search process can be BFS or DFS while each state is evaluated by a classifier (via a prompt) or majority vote.\\n\\n\\nFig. 2. How Self-Ask works with external search queries.(Image source: Yao et al. 2022).\\nAutomatic Prompt Design#\\nPrompt is a sequence of prefix tokens that increase the probability of getting  desired output given input. Therefore we can treat them as trainable parameters and optimize them directly on the embedding space via gradient descent, such as AutoPrompt (Shin et al., 2020, Prefix-Tuning (Li & Liang (2021)), P-tuning (Liu et al. 2021) and Prompt-Tuning (Lester et al. 2021). This section in my “Controllable Neural Text Generation” post has a good coverage of them. The trend from AutoPrompt to Prompt-Tuning is that the setup gets gradually simplified.\\nAPE (Automatic Prompt Engineer; Zhou et al. 2022) is a method to search over a pool of model-generated instruction candidates and then filters the candidate set according to a chosen score function to ultimately choose the best candidate with highest score.\\n\\n\\nPrompt LLM to generate instruction candidates based on a small set of demonstrations in the form of input-output pairs. E.g. {{Given desired input-output pairs}}\\\\n\\\\nThe instruction is.\\n\\n\\nGiven a dataset of $\\\\mathcal{D}_\\\\text{train} = \\\\{(x, y)\\\\}$, we would like to find an instruction $\\\\rho$ such that $\\\\rho^* = \\\\arg\\\\max_\\\\rho \\\\mathbb{E}_{(x, y) \\\\in \\\\mathcal{D}_\\\\text{train}} [f(\\\\rho, x, y)]$, where $f(.)$ is a per-sample score function, such as execution accuracy $\\\\mathbb{1}[\\\\text{LM}(.\\\\vert \\\\rho, x)=y]$ or log probability: $p_\\\\text{LM}(y \\\\mid \\\\rho, x)$.'}\n",
      "\n",
      "\n",
      "\u001b[92mGrader Trial :0\u001b[0m\n",
      "\u001b[92mGrader Trial :1\u001b[0m\n",
      "Node :Grader:\n",
      "State :{'Grade': 'yes'}\n",
      "\n",
      "\n",
      "\u001b[92mReporter\u001b[0m\n",
      "Node :Reporter:\n",
      "State :{'Generation': ' Prompt Engineering refers to the process of designing and optimizing prompts (sequences of tokens) to increase the likelihood of getting a desired output from a language model given an input. This can be done by treating prompts as trainable parameters and optimizing them directly on the embedding space via gradient descent, or by using methods like Automatic Prompt Engineer (APE) to search over a pool of model-generated instruction candidates. The goal is to find an instruction that maximizes the expected score function for a given dataset of input-output pairs.'}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run\n",
    "inputs = {\n",
    "    \"Question\": \"What Prompt Enegenering?\"\n",
    "}\n",
    "for output in ARAG_Graph.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        print(f\"Node :{key}:\")\n",
    "        print(f\"State :{value}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Prompt Engineering refers to the process of designing and optimizing prompts (sequences of tokens) to increase the likelihood of getting a desired output from a language model given an input. This can be done by treating prompts as trainable parameters and optimizing them directly on the embedding space via gradient descent, or by using methods like Automatic Prompt Engineer (APE) to search over a pool of model-generated instruction candidates. The goal is to find an instruction that maximizes the expected score function for a given dataset of input-output pairs.\n"
     ]
    }
   ],
   "source": [
    "# Final generation\n",
    "print(value[\"Generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
