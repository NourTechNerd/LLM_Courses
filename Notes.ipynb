{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM App Levels:\n",
    "\n",
    "<img src=\"Images\\Image1.png\" width=\"400\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Q&A LLM      :** Just Quation and Answer LLM.\n",
    "- **Chatbot      :** \n",
    "    1. Q&A + Short Term memory (The LLM have a context or short memeory)\n",
    "    2. The *Context Window* define the size of this memeory, is the amount of tokens that the \n",
    "       LLM can process in the same time.\n",
    "       - Contex Window Large  => More coputational Cost + More Time + LLM can Lose the Context.\n",
    "       - Contetx Window Small => Response Not relevent to the context !!! \n",
    "- **RAG Chatbot  :** \n",
    "    1. Chatbot+ Externel Data => Chatbot Have more knowledge in a domaine.\n",
    "    2. We Augmete what the LLM generare with a relevent data.\n",
    "- **LLM Agent(s)  :** Chatbot + Tools (Like Python interepter,APIs, Terminal,Browser...)\n",
    "- **LLM OS  :** Chatbot + RAG +Tools (Like Python interepter,APIs, Terminal,Browser...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Hacker Video:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Lauguadge Model :**  Is a model that can predect the next token based on the previous tokens.\n",
    "- **Token :**  Is a sequence of caracters (character,word ,subword...)\n",
    "- **How to Calculate VRAM we need ?**\n",
    "  - For example Llama2-7b , with a precesion of float16 bit --> 2 bytes --> 14 Gigabyte.\n",
    "---> So we need to 14G in The Vram just to load weights.\n",
    "- **Quantization ?** \n",
    "  - On LLM is a technique that reduce the precesion of model's weights in order to reduce the memory cost.\n",
    "  - There are Multiple ways to quantize like : Calibration,affine quantization,GPTQ...\n",
    "- **Instruction Tuning**\n",
    "  - Is a type of finetuning based on Instruction-output dataset ,the LLM map this data and learn to provide more specefic responces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG With LangChain:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Chunking/Spliting:\n",
    "  - We split the text from all documnents into small splits,(Because the Embedding model has a limited Context Window).\n",
    "    - Fixed Size Chunking.\n",
    "    - Sentence based Chunking.(Use chracters like  . ? ! ...)\n",
    "    - Recursive Chunking. (Use chracters like \\n \\n\\n \\t ...)\n",
    "    - Embedding Based Chunking.\n",
    "2. Embedding:\n",
    "  - We just Embed the splits we got from Chunkig step.\n",
    "3. Indexing:\n",
    "  - We try to orginize Vectors in a way that accelerate searching for relevent information.\n",
    "    - Using KNN :\n",
    "      - We Search for the K nearest neighbors using a Similarity metric (often Cosine Similarity).\n",
    "    - Using Clustring:\n",
    "      - Create Clusters , then search the nearest Centroid to the question Vector,then search inside the Cluster for the relevents Splits.\n",
    "  - To do this we ofen use **Choroma Database** is a Vector database, suitble for working with embbeding (Storing , Searching...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality Of RAG System :\n",
    "\n",
    "Its depend on :\n",
    "- The Quality of Information and Documents.\n",
    "- The Text Spliting technic.\n",
    "- The Searching technic (KNN, Clustring...)\n",
    "- The LLM used and the Embedding model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langraph :\n",
    "\n",
    "<img src=\"Images\\Graph_Example.png\" width=\"600\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Graph :** in Langgraph is a representation of the interaction between the LLMs and tools.\n",
    "  - Node : LLM or Tool\n",
    "  - Edge : Connection .\n",
    "  - Feedback Edge : an Edge with a specific Condition.\n",
    "\n",
    "<img src=\"Images\\State_Example.png\" width=\"600\" height=\"300\">\n",
    "\n",
    "- **State :** Is like a history page where we write what happens in the graph, for example:\n",
    "  - The URLs+Content we got from the WebSearch.\n",
    "  - The Scrapping of a specific URL.\n",
    "  - The Report of the Reporter\n",
    "  - The Feedback of the Reviewer...\n",
    "- Nodes Commenicate between each outhre by writing and reading from the State object\n",
    "- **Why The State ?**\n",
    "  - Keep Tracking to what happens in the Graph.\n",
    "  - The agents can communicate between each outher,for exemple if the Reviewer refuse the Report the Researcher must Know that to enhance his resarch.\n",
    "- **Some Tips :**\n",
    "  - The Fist thing to build our Agents System is , define how this agents will work together (The Graph) and what informations you want to reccord (The state)\n",
    "- **Agent RunTime:**\n",
    "  - Agent in a loop ,Take an action--> The result--> Take the result as input for next action.\n",
    "\n",
    "- **In Paractise** \n",
    "  - A **Node** will be a Python function and a **State** Will be a Python Dictionary.\n",
    "  - An **edge** between tow Nodes for example A-->B ,means that every time the function associeted to Node A executed the Function for B will automaticaly executed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In **Langraph** there 3 pricipale Types if messages recorded in the State:\n",
    "  - **HummanMessage :** the prompt of the User.\n",
    "  - **AIMessage     :** the response of the LLM.\n",
    "  - **ToolMessage   :** the response of the tool (API , simple function...)\n",
    "  - **SystemMessage :** Like SystemPropmt used to custimauze the LLM responses.\n",
    "- Langraph update automaticaly the State with New messages, but the Node Output should be in the form of a dictionary with key \"mesages\"\n",
    "- The Name of a node should be Unique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Calling :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Allows The LLM to request a specific tool and is coming up with the arguments to a tool.\n",
    "- We Structre the Response of an LLM in a way we can use it to call a tool.\n",
    "- To use Ollama we need to search for LLM that are the best in **Tool-Use** Bech, to give good result with the tools like :\n",
    "    - Mistral:7b V0.3\n",
    "    - llama3-groq-tool-use:70b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langraph Memory :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Adding a memory is very important for our Agent , so his reponses will be relevent to the context of the Conversation with the User.\n",
    "- We will create a simple graph like Part 1 , but this time we wil add a langraph memory to our Agent\n",
    "  - **How ?**\n",
    "    - It simple ,this memory is like a database (sqlite...),where we store States every one has a unique **thread_id**.So every time the graph executed he loads the State using  its specific id."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human in the Loop :\n",
    "- The concept is easy, with the amazing langGraph we can Stop ▶️ the execution of the graph at any step want and inspect for example the State messages to see if everything as expected, then Resume ⏸ the graph execution from the step he stoped.\n",
    "- Updating The graph manualy:\n",
    "  - This capability is particularly useful when you want to correct the agent's mistakes, explore alternative paths, or guide the agent towards a specific goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangServe :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The difference between invoke,stream,batch:\n",
    "   - **runnuble.invoke()** : Take one input, the output after the runnuble finish his work.\n",
    "   - **runnuble.stream()** : Take one input, the output is generated while the runnuble works\n",
    "   - **runnuble.batch()**  : Take multiple inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⛔️ APIs:\n",
    "\n",
    "- Always remember that using APIs for LLMs or VLMs or any model is a data privacy issue.\n",
    "\n",
    "- Free API or even paid one means You pay by your **data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Hero",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
